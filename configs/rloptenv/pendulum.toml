[config]
save_dir="results/rloptenv_pend"
exp_file="experiment/experiment.jl"
exp_module_name = "Main"
exp_func_name = "run_experiment"
pre_exp_files = []
arg_iter_type = "iter"
post_exp = "gen_metric_dict"

[static_args]
init_num_episodes = 1
init_policy = "default"
num_episodes = 1000
num_grad_steps = 1
num_env_steps = 1
batch_size = 1
max_num_episodes = 256
max_episode_length = 200
history_window = 1
BufferType = "TransitionReplayBuffer"
hidden_size = 128
activation = "relu"
num_layers = -1
gamma = 0.99
optimizer = "ADAM"
replan = 0
bootstrap = true
curriculum = false
overlap = true
shaping = false
force = "online"
gpu = true
corruption_rate = 0.0
drop_rate = 0.0
reg = false
continuing = false
AgentType = "MCValue"
recurrent_action_dim = 0
update_freq = 100
predict_window = 0
behavior = "default"
run_traj = false
measurement_funcs = """
        online_returns,
        # buffer_loss,
        estimate_startvalue,
        # estimate_termvalue,
        mc_buffer_loss,
        optimize_student_metrics,
        # rollout_returns,
        # rollout_returns_wide,
        # rollout_returns_narrow,
        # rollout_returns_tanh,
        # rollout_returns_eval_env,
        # rollout_returns_constant,
        # action_gap,
"""
callback_funcs = """
        # optimize_student,
"""

[sweep_args]
# lr = "[0.001]"
lr = "[0.0005]"
seed = "using Random; rand(MersenneTwister(2313292115), 1:10^10, 10)"
# state_representation = ["PE-xon_200", "PE-x_50"]
state_representation = ["PE-xon_200"]
# state_representation = ["PE-xon_20", "PE-xon_200", "PE-xon2-200"]
EnvType = ["Pendulum_RLOptEnv"]
# EnvType = ["Pendulum_RLOptEnv", "Pendulum_RLOptEnv-hardexplore"]
# EnvType = ["Pendulum_RLOptEnv-hardexplore"]
# pooling_func = ["mean", "attention"]
pooling_func = ["mean"]
